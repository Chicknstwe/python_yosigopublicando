{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo práctico: conteo de palabras en tweets y elaboración de nube de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este documento se utiliza el fichero excel 'all_tweets.xlsx'. Se trata de una base de datos de tweets de una serie de medios de comunicación españoles que han usado hashtags relacionados con la crisis del coronavirus durante abril y mayo de 2020. Este fichero se ha generado mediante el acceso a la API de Twitter con un script en Python. A lo largo de este documento se explica el funcionamiento de un script para realizar un conteo de palabras de los tweets y elaborar una [una nube de palabras](https://es.wikipedia.org/wiki/Nube_de_palabras) con éstas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las tres siguientes celdas deben ser ejecutada en caso de que no estén instalados los paquetes `xlrd`, `pandas`, `matplotlib` o `wordcloud`. En caso de que ya estén instalados, simplemente se omiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero es importar los paquetes que se pretende usar a lo largo del documento y definir la función auxiliar `get_col_index_by_name`. Esta función nos ayudará a identificar el índice de la columna que contiene los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_col_index_by_name(cols, name):\n",
    "    \n",
    "    i = 0\n",
    "    for col in cols:\n",
    "        if col.value == 'full_text':\n",
    "            return i\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda se abre el fichero excel por la hoja que contiene los datos y se identifica el índice de la columna que contiene los tweets.\n",
    "\n",
    "En el siguiente fragmento de código se usa el módulo `xlrd`:\n",
    "* Sobre el [módulo xlrd](https://xlrd.readthedocs.io/en/latest/api.html#xlrd-sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'all_tweets.xlsx'\n",
    "\n",
    "wb = xlrd.open_workbook(file) \n",
    "sheet = wb.sheet_by_index(0)\n",
    "index = get_col_index_by_name(sheet.row(0), 'full_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bucle realiza las siguientes tareas:\n",
    "1. Retira de cada tweet los saltos de línea, las palabras y ciertos caracteres que no nos interesan, como 'para', 'sobre' o los dos puntos (':').\n",
    "1. Se divide cada tweet en palabras y se añaden a una lista.\n",
    "1. Se añaden las palabras de cada tweet al objeto string `words_cloud` para formar la nube de palabras posteriormente.\n",
    "1. Se hace un conteo de las palabras de cada tweet y se recoge la información en el diccionario `words`.\n",
    "\n",
    "\n",
    "En este fragmento de código se utilizan métodos de la clase `str`, la función defaultdict() y comprehensiones de lista:\n",
    "* Sobre [el método `str.replace()`](https://docs.python.org/3/library/stdtypes.html#str.replace).\n",
    "* Sobre [el método `str.split()`](https://docs.python.org/3/library/stdtypes.html#str.split).\n",
    "* Sobre [el método `str.lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower).\n",
    "* Sobre [la función `defaultdict()`](https://docs.python.org/3.8/library/collections.html#collections.defaultdict).\n",
    "* Sobre las [comprehensiones de lista](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = sheet.col_values(index)[1:]\n",
    "words = defaultdict(lambda:0)\n",
    "words_cloud = ''\n",
    "for tweet in tweets:\n",
    "    \n",
    "    tweet = tweet.replace('\\n', ' ').lower()\n",
    "    \n",
    "    useless = [',', '.', ':', ';', 'para', 'según', 'por', 'desde', 'sobre', 'hasta', 'como', 'contra', 'este', 'tras']\n",
    "    for uw in useless:\n",
    "        tweet = tweet.replace(uw, '')\n",
    "        \n",
    "    tw_words = [word for word in tweet.split(' ') if len(word) > 3 and word[:4] != 'http' and word[0] != '@' and word[0] != '#']\n",
    "    \n",
    "    words_cloud += ' '.join(tw_words) + ' '\n",
    "    \n",
    "    for word in tw_words:\n",
    "        words[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya casi hemos terminado. El paquete `pandas` nos ofrece una gran variedad de herramientas para trabajar con datos, así que en el siguiente fragmento de código vamos a cambiar la estructura de nuestros datos para poder trabajar con objetos `DataFrame`. En el objeto `DataFrame` que hemos creado, la columna `word` contiene las palabras y la columna `n` contiene el número de veces que cada palabra aparece en los tweets.\n",
    "\n",
    "En el siguiente fragmento se emplean comprehensiones de lista, la función `zip` y recursos propios del paquete `pandas`:\n",
    "* Sobre [la función `zip`](https://docs.python.org/3/library/functions.html#zip).\n",
    "* Sobre [el paquete `pandas`](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = [x for x in zip(words.keys(), words.values())]\n",
    "\n",
    "data = pd.DataFrame(data_words)\n",
    "data.columns = ['word', 'n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el resultado. El método `DataFrame.nlargest()` nos permite seleccionar las filas con los valores más altos de la variable que especifiquemos. En este caso le estamos indicando que nos muestre las 40 palabras que tienen un conteo más alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.nlargest(40, ['n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos visto las palabras más comunes procedemos a elaborar la nube de palabras. En el siguiente fragmento de código se usa `WordCloud` para definir las características de la nube y con `plt`, nuevo nombre con el que hemos definido `matplotlib.pylot` por comodidad, se construye el gráfico. Veamos cómo ha quedado.\n",
    "\n",
    "En este fragmento se usan los siguientes paquetes:\n",
    "* Sobre [el paquete `wordcloud`](https://pypi.org/project/wordcloud/)\n",
    "* Sobre [el paquete `matplotlib`](https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, \n",
    "                      height = 800, \n",
    "                      background_color ='white', \n",
    "                      min_font_size = 10).generate(words_cloud)\n",
    "\n",
    "plt.figure(figsize = (7, 7), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
